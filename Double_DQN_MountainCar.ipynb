{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSExFYIi4U-R"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLGOeQZ74U-X"
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JFDEPyC94U-a",
    "outputId": "b1b3b562-6363-4ff7-bf16-52d33a05ba88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size= env.observation_space.shape[0]\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7d7TuvNG4U-d",
    "outputId": "8c944095-27eb-4668-b321-2ea62bebadab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size= env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xDHmlNC4U-g"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa-sP1lh4U-k"
   },
   "outputs": [],
   "source": [
    "n_episodes= 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULJuSeKF4U-m"
   },
   "outputs": [],
   "source": [
    "output_dir= 'model_output/MountainCar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NneZmMaQ4U-p"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cizjkV1k4U-s"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.env= env\n",
    "        self.state_size= state_size\n",
    "        self.action_size= action_size\n",
    "        \n",
    "        self.memory= deque(maxlen=200000)\n",
    "        \n",
    "        self.gamma= 0.99\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay= .85\n",
    "        self.epsilon_min=0.00001\n",
    "        \n",
    "        self.learning_rate= 0.001251\n",
    "        self.model= self._build_model()\n",
    "        self.target_model=self._build_model()\n",
    "        \n",
    "        self.update_target_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model= tf.keras.models.Sequential()\n",
    "        state_shape= self.env.observation_space.shape\n",
    "        model.add(tf.keras.layers.Dense(24, input_shape= state_shape, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(48, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    def act(self, state):\n",
    "        if np.random.rand(1) <=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values= self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states=[]\n",
    "        targets=[]\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target=reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]))\n",
    "            target_f= self.model.predict(state)\n",
    "            target_f[0][action]= target\n",
    "            states.append(state[0])\n",
    "            targets.append(target_f[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "            \n",
    "        \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TURoYt0U4U-w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Colin\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Colin\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "agent= DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DNrbYkJp4U-1",
    "outputId": "93c100e2-c038-4f03-8766-47c6cc4feca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Colin\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "episode: 0/70000, score: 199, e 1.0, help: [[-0.53327205 -0.01343278]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 1/70000, score: 199, e 0.85, help: [[-0.63251497  0.01193769]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 2/70000, score: 199, e 0.72, help: [[-0.20412662  0.00235219]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 3/70000, score: 199, e 0.61, help: [[-0.84518602  0.0045429 ]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 4/70000, score: 199, e 0.52, help: [[-0.61302877 -0.00153696]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 5/70000, score: 199, e 0.44, help: [[-0.46991779 -0.01549339]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 6/70000, score: 199, e 0.38, help: [[-0.63693088  0.02154086]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 7/70000, score: 199, e 0.32, help: [[-0.20043436  0.03310686]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 8/70000, score: 199, e 0.27, help: [[-0.49212264 -0.01359907]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 9/70000, score: 199, e 0.23, help: [[-0.23574512  0.01299286]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 10/70000, score: 199, e 0.2, help: [[-0.8137998  -0.02642325]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 11/70000, score: 199, e 0.17, help: [[-0.60175023  0.02547423]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 12/70000, score: 199, e 0.14, help: [[-0.49674577 -0.00828131]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 13/70000, score: 199, e 0.12, help: [[-0.58296684 -0.00905091]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 14/70000, score: 199, e 0.1, help: [[-0.24653622  0.04377364]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 15/70000, score: 199, e 0.087, help: [[-0.67229946  0.02073797]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 16/70000, score: 199, e 0.074, help: [[-0.41211925 -0.01183192]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 17/70000, score: 199, e 0.063, help: [[-0.86061481  0.02020908]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 18/70000, score: 199, e 0.054, help: [[-0.57373525  0.00121406]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 19/70000, score: 199, e 0.046, help: [[-0.20348598  0.02787408]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 20/70000, score: 199, e 0.039, help: [[-0.6608822   0.02885268]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 21/70000, score: 199, e 0.033, help: [[-0.52259146 -0.01218286]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 22/70000, score: 199, e 0.028, help: [[-0.40952464 -0.00321878]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 23/70000, score: 199, e 0.024, help: [[-0.21244296  0.03179258]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 24/70000, score: 199, e 0.02, help: [[-0.63665263  0.00618018]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 25/70000, score: 199, e 0.017, help: [[-0.75029751  0.02674483]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 26/70000, score: 199, e 0.015, help: [[-0.34404877  0.03198646]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 27/70000, score: 199, e 0.012, help: [[-0.27116993 -0.00870188]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 28/70000, score: 199, e 0.011, help: [[0.15915451 0.01998926]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 29/70000, score: 199, e 0.009, help: [[-0.30740084 -0.05953433]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 30/70000, score: 199, e 0.0076, help: [[-0.64366019  0.00822434]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 31/70000, score: 199, e 0.0065, help: [[-0.48993404 -0.00492114]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 32/70000, score: 199, e 0.0055, help: [[-0.3213821   0.02002825]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 33/70000, score: 199, e 0.0047, help: [[0.04715954 0.03310604]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 34/70000, score: 199, e 0.004, help: [[-0.21040575  0.04483284]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 35/70000, score: 199, e 0.0034, help: [[ 0.10089105 -0.00338989]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 36/70000, score: 199, e 0.0029, help: [[-0.87058387 -0.04463507]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 37/70000, score: 199, e 0.0024, help: [[-1.07120772  0.02299705]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 38/70000, score: 199, e 0.0021, help: [[-9.70130711e-01 -3.67957534e-04]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 39/70000, score: 199, e 0.0018, help: [[-1.07419049 -0.02359636]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 40/70000, score: 199, e 0.0015, help: [[-0.309145  -0.0551539]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 41/70000, score: 199, e 0.0013, help: [[-0.41445007  0.00938529]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 42/70000, score: 199, e 0.0011, help: [[-1.11889085  0.01689551]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 43/70000, score: 199, e 0.00092, help: [[-0.31061274  0.01555926]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 44/70000, score: 199, e 0.00078, help: [[-0.56260851 -0.01771508]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 45/70000, score: 199, e 0.00067, help: [[-0.56916344 -0.0529532 ]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 46/70000, score: 199, e 0.00057, help: [[-0.05348667  0.00321236]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 47/70000, score: 199, e 0.00048, help: [[-0.75985986  0.01077814]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 48/70000, score: 199, e 0.00041, help: [[-0.98759097  0.03636572]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 49/70000, score: 199, e 0.00035, help: [[-0.33099132 -0.00600398]], reward: -1.0, 100score avg: -199.0\n",
      "episode: 50/70000, score: 172, e 0.0003, help: [[0.5046896 0.0249762]], reward: -1.0, 100score avg: -198.47058823529412\n",
      "episode: 51/70000, score: 199, e 0.00025, help: [[-0.19551247  0.03495141]], reward: -1.0, 100score avg: -198.48076923076923\n",
      "episode: 52/70000, score: 199, e 0.00021, help: [[-0.69118052 -0.02262667]], reward: -1.0, 100score avg: -198.49056603773585\n",
      "episode: 53/70000, score: 199, e 0.00018, help: [[-0.56059699  0.00935514]], reward: -1.0, 100score avg: -198.5\n",
      "episode: 54/70000, score: 199, e 0.00015, help: [[-0.24268297  0.04713885]], reward: -1.0, 100score avg: -198.5090909090909\n",
      "episode: 55/70000, score: 199, e 0.00013, help: [[-1.2  0. ]], reward: -1.0, 100score avg: -198.51785714285714\n",
      "episode: 56/70000, score: 199, e 0.00011, help: [[-0.95286981 -0.03007782]], reward: -1.0, 100score avg: -198.52631578947367\n",
      "episode: 57/70000, score: 199, e 9.5e-05, help: [[-0.78729361  0.01194222]], reward: -1.0, 100score avg: -198.5344827586207\n",
      "episode: 58/70000, score: 199, e 8.1e-05, help: [[-0.51680662 -0.00723163]], reward: -1.0, 100score avg: -198.54237288135593\n",
      "episode: 59/70000, score: 199, e 6.9e-05, help: [[-0.36648116 -0.02715398]], reward: -1.0, 100score avg: -198.55\n",
      "episode: 60/70000, score: 116, e 5.8e-05, help: [[0.50305978 0.02950577]], reward: -1.0, 100score avg: -197.19672131147541\n",
      "episode: 61/70000, score: 141, e 4.9e-05, help: [[0.52164569 0.04206392]], reward: -1.0, 100score avg: -196.29032258064515\n",
      "episode: 62/70000, score: 199, e 4.2e-05, help: [[-0.20384988  0.0602127 ]], reward: -1.0, 100score avg: -196.33333333333334\n",
      "episode: 63/70000, score: 199, e 3.6e-05, help: [[-0.35689913 -0.02551394]], reward: -1.0, 100score avg: -196.375\n",
      "episode: 64/70000, score: 199, e 3e-05, help: [[-0.38241989  0.03702971]], reward: -1.0, 100score avg: -196.41538461538462\n",
      "episode: 65/70000, score: 199, e 2.6e-05, help: [[-0.23769569 -0.0319059 ]], reward: -1.0, 100score avg: -196.45454545454547\n",
      "episode: 66/70000, score: 199, e 2.2e-05, help: [[-0.12102362  0.02487071]], reward: -1.0, 100score avg: -196.49253731343285\n",
      "episode: 67/70000, score: 199, e 1.9e-05, help: [[-0.16146372 -0.01782518]], reward: -1.0, 100score avg: -196.52941176470588\n",
      "episode: 68/70000, score: 199, e 1.6e-05, help: [[-0.38675274  0.06236465]], reward: -1.0, 100score avg: -196.56521739130434\n",
      "episode: 69/70000, score: 199, e 1.3e-05, help: [[-0.18077894 -0.01434667]], reward: -1.0, 100score avg: -196.6\n",
      "episode: 70/70000, score: 199, e 1.1e-05, help: [[-0.12371769  0.05379852]], reward: -1.0, 100score avg: -196.6338028169014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 71/70000, score: 199, e 9.7e-06, help: [[-0.74258636  0.01068138]], reward: -1.0, 100score avg: -196.66666666666666\n",
      "episode: 72/70000, score: 199, e 9.7e-06, help: [[-0.59745224  0.0393401 ]], reward: -1.0, 100score avg: -196.6986301369863\n",
      "episode: 73/70000, score: 199, e 9.7e-06, help: [[-0.32718423 -0.05361774]], reward: -1.0, 100score avg: -196.72972972972974\n",
      "episode: 74/70000, score: 199, e 9.7e-06, help: [[-0.02702674  0.01304403]], reward: -1.0, 100score avg: -196.76\n",
      "episode: 75/70000, score: 199, e 9.7e-06, help: [[-0.96336355  0.00618102]], reward: -1.0, 100score avg: -196.78947368421052\n",
      "episode: 76/70000, score: 199, e 9.7e-06, help: [[0.24714812 0.03056357]], reward: -1.0, 100score avg: -196.8181818181818\n",
      "episode: 77/70000, score: 199, e 9.7e-06, help: [[-0.2956503   0.01402103]], reward: -1.0, 100score avg: -196.84615384615384\n",
      "episode: 78/70000, score: 199, e 9.7e-06, help: [[-0.27539099  0.02907886]], reward: -1.0, 100score avg: -196.873417721519\n",
      "episode: 79/70000, score: 199, e 9.7e-06, help: [[-0.08080867 -0.01999756]], reward: -1.0, 100score avg: -196.9\n",
      "episode: 80/70000, score: 199, e 9.7e-06, help: [[-0.70131588 -0.00690163]], reward: -1.0, 100score avg: -196.92592592592592\n",
      "episode: 81/70000, score: 199, e 9.7e-06, help: [[-0.38556698  0.05112017]], reward: -1.0, 100score avg: -196.9512195121951\n",
      "episode: 82/70000, score: 199, e 9.7e-06, help: [[-0.24508422  0.00848237]], reward: -1.0, 100score avg: -196.97590361445782\n",
      "episode: 83/70000, score: 199, e 9.7e-06, help: [[-0.03859626 -0.0011332 ]], reward: -1.0, 100score avg: -197.0\n",
      "episode: 84/70000, score: 199, e 9.7e-06, help: [[-0.70890384  0.04865804]], reward: -1.0, 100score avg: -197.0235294117647\n",
      "episode: 85/70000, score: 199, e 9.7e-06, help: [[ 0.20748517 -0.00453486]], reward: -1.0, 100score avg: -197.04651162790697\n",
      "episode: 86/70000, score: 199, e 9.7e-06, help: [[-0.53716643 -0.02763483]], reward: -1.0, 100score avg: -197.06896551724137\n",
      "episode: 87/70000, score: 180, e 9.7e-06, help: [[0.51286876 0.0180566 ]], reward: -1.0, 100score avg: -196.875\n",
      "episode: 88/70000, score: 199, e 9.7e-06, help: [[-0.75082044  0.03490297]], reward: -1.0, 100score avg: -196.8988764044944\n",
      "episode: 89/70000, score: 153, e 9.7e-06, help: [[0.51581214 0.01950497]], reward: -1.0, 100score avg: -196.4111111111111\n",
      "episode: 90/70000, score: 199, e 9.7e-06, help: [[-0.4874426   0.00493961]], reward: -1.0, 100score avg: -196.43956043956044\n",
      "episode: 91/70000, score: 199, e 9.7e-06, help: [[-0.61124669 -0.00994728]], reward: -1.0, 100score avg: -196.4673913043478\n",
      "episode: 92/70000, score: 199, e 9.7e-06, help: [[-0.00068502  0.00707634]], reward: -1.0, 100score avg: -196.49462365591398\n",
      "episode: 93/70000, score: 199, e 9.7e-06, help: [[-0.28356949  0.03740098]], reward: -1.0, 100score avg: -196.5212765957447\n",
      "episode: 94/70000, score: 199, e 9.7e-06, help: [[-1.15678137 -0.01652516]], reward: -1.0, 100score avg: -196.54736842105262\n",
      "episode: 95/70000, score: 199, e 9.7e-06, help: [[-0.56921958  0.0298749 ]], reward: -1.0, 100score avg: -196.57291666666666\n",
      "episode: 96/70000, score: 183, e 9.7e-06, help: [[0.50290374 0.04743347]], reward: -1.0, 100score avg: -196.43298969072166\n",
      "episode: 97/70000, score: 174, e 9.7e-06, help: [[0.51793116 0.01896483]], reward: -1.0, 100score avg: -196.20408163265307\n",
      "episode: 98/70000, score: 199, e 9.7e-06, help: [[-0.67467709  0.0349684 ]], reward: -1.0, 100score avg: -196.23232323232324\n",
      "episode: 99/70000, score: 199, e 9.7e-06, help: [[-0.70880826  0.04573221]], reward: -1.0, 100score avg: -196.26\n",
      "episode: 100/70000, score: 196, e 9.7e-06, help: [[0.51328349 0.03315665]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 101/70000, score: 199, e 9.7e-06, help: [[-0.40341602  0.00623414]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 102/70000, score: 199, e 9.7e-06, help: [[-0.68820064  0.02804275]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 103/70000, score: 199, e 9.7e-06, help: [[-0.23583623 -0.02922105]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 104/70000, score: 199, e 9.7e-06, help: [[-0.89646345  0.024044  ]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 105/70000, score: 199, e 9.7e-06, help: [[-0.1730356   0.03361778]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 106/70000, score: 199, e 9.7e-06, help: [[-0.36223836  0.02965995]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 107/70000, score: 199, e 9.7e-06, help: [[-0.03255081  0.03561949]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 108/70000, score: 199, e 9.7e-06, help: [[-0.70880826  0.04573221]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 109/70000, score: 199, e 9.7e-06, help: [[-0.80968768 -0.03093847]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 110/70000, score: 199, e 9.7e-06, help: [[-0.89174955 -0.02806841]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 111/70000, score: 199, e 9.7e-06, help: [[-0.13190357 -0.02377956]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 112/70000, score: 199, e 9.7e-06, help: [[-1.0335885   0.02429471]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 113/70000, score: 199, e 9.7e-06, help: [[-0.08791959 -0.00700638]], reward: -1.0, 100score avg: -196.23\n",
      "episode: 114/70000, score: 197, e 9.7e-06, help: [[0.52999779 0.03303068]], reward: -1.0, 100score avg: -196.21\n",
      "episode: 115/70000, score: 199, e 9.7e-06, help: [[-0.4886605  -0.01682498]], reward: -1.0, 100score avg: -196.21\n",
      "episode: 116/70000, score: 167, e 9.7e-06, help: [[0.50050345 0.02221268]], reward: -1.0, 100score avg: -195.89\n",
      "episode: 117/70000, score: 199, e 9.7e-06, help: [[-0.40545968  0.02040798]], reward: -1.0, 100score avg: -195.89\n",
      "episode: 118/70000, score: 187, e 9.7e-06, help: [[0.51573398 0.03747321]], reward: -1.0, 100score avg: -195.77\n",
      "episode: 119/70000, score: 199, e 9.7e-06, help: [[-0.28358624 -0.00203512]], reward: -1.0, 100score avg: -195.77\n",
      "episode: 120/70000, score: 199, e 9.7e-06, help: [[-0.12011031 -0.00018799]], reward: -1.0, 100score avg: -195.77\n",
      "episode: 121/70000, score: 199, e 9.7e-06, help: [[-0.52725392  0.02405421]], reward: -1.0, 100score avg: -195.77\n",
      "episode: 122/70000, score: 199, e 9.7e-06, help: [[-0.34914832 -0.00924398]], reward: -1.0, 100score avg: -195.77\n",
      "episode: 123/70000, score: 199, e 9.7e-06, help: [[-0.41820836 -0.01465074]], reward: -1.0, 100score avg: -195.77\n",
      "episode: 124/70000, score: 199, e 9.7e-06, help: [[-0.79135435 -0.0407053 ]], reward: -1.0, 100score avg: -195.77\n",
      "episode: 125/70000, score: 186, e 9.7e-06, help: [[0.5148259 0.042368 ]], reward: -1.0, 100score avg: -195.64\n",
      "episode: 126/70000, score: 163, e 9.7e-06, help: [[0.50220754 0.01596303]], reward: -1.0, 100score avg: -195.28\n",
      "episode: 127/70000, score: 199, e 9.7e-06, help: [[-0.31980832 -0.03699948]], reward: -1.0, 100score avg: -195.28\n",
      "episode: 128/70000, score: 199, e 9.7e-06, help: [[0.40253499 0.01535211]], reward: -1.0, 100score avg: -195.28\n",
      "episode: 129/70000, score: 159, e 9.7e-06, help: [[0.51455509 0.02467396]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 130/70000, score: 199, e 9.7e-06, help: [[-0.38399608 -0.00436982]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 131/70000, score: 199, e 9.7e-06, help: [[-0.65332535  0.02215548]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 132/70000, score: 199, e 9.7e-06, help: [[-0.05742029  0.03863894]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 133/70000, score: 199, e 9.7e-06, help: [[-0.71214092  0.01799988]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 134/70000, score: 199, e 9.7e-06, help: [[0.27447769 0.04005121]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 135/70000, score: 199, e 9.7e-06, help: [[0.50831055 0.03509704]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 136/70000, score: 199, e 9.7e-06, help: [[-0.4094143  -0.03242052]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 137/70000, score: 199, e 9.7e-06, help: [[-0.53990017  0.01745665]], reward: -1.0, 100score avg: -194.88\n",
      "episode: 138/70000, score: 82, e 9.7e-06, help: [[0.51251644 0.0248038 ]], reward: -1.0, 100score avg: -193.71\n",
      "episode: 139/70000, score: 137, e 9.7e-06, help: [[0.5107289  0.02314483]], reward: -1.0, 100score avg: -193.09\n",
      "episode: 140/70000, score: 199, e 9.7e-06, help: [[-0.99041817 -0.00522953]], reward: -1.0, 100score avg: -193.09\n",
      "episode: 141/70000, score: 199, e 9.7e-06, help: [[-0.42237403 -0.03226153]], reward: -1.0, 100score avg: -193.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 142/70000, score: 199, e 9.7e-06, help: [[-0.42989512 -0.03545322]], reward: -1.0, 100score avg: -193.09\n",
      "episode: 143/70000, score: 199, e 9.7e-06, help: [[-0.35451704  0.0601173 ]], reward: -1.0, 100score avg: -193.09\n",
      "episode: 144/70000, score: 199, e 9.7e-06, help: [[ 0.36308517 -0.00177309]], reward: -1.0, 100score avg: -193.09\n",
      "episode: 145/70000, score: 199, e 9.7e-06, help: [[-0.70813853 -0.0135698 ]], reward: -1.0, 100score avg: -193.09\n",
      "episode: 146/70000, score: 191, e 9.7e-06, help: [[0.50128423 0.02320023]], reward: -1.0, 100score avg: -193.01\n",
      "episode: 147/70000, score: 199, e 9.7e-06, help: [[-0.27969318  0.0162373 ]], reward: -1.0, 100score avg: -193.01\n",
      "episode: 148/70000, score: 199, e 9.7e-06, help: [[-0.0793114  -0.02657818]], reward: -1.0, 100score avg: -193.01\n",
      "episode: 149/70000, score: 199, e 9.7e-06, help: [[-0.66586801  0.02013104]], reward: -1.0, 100score avg: -193.01\n",
      "episode: 150/70000, score: 199, e 9.7e-06, help: [[-0.72310569 -0.00159812]], reward: -1.0, 100score avg: -193.28\n",
      "episode: 151/70000, score: 193, e 9.7e-06, help: [[0.50375369 0.01991219]], reward: -1.0, 100score avg: -193.22\n",
      "episode: 152/70000, score: 199, e 9.7e-06, help: [[-0.15524608  0.00114321]], reward: -1.0, 100score avg: -193.22\n",
      "episode: 153/70000, score: 194, e 9.7e-06, help: [[0.5368578  0.04995707]], reward: -1.0, 100score avg: -193.17\n",
      "episode: 154/70000, score: 159, e 9.7e-06, help: [[0.50150832 0.03584265]], reward: -1.0, 100score avg: -192.77\n",
      "episode: 155/70000, score: 199, e 9.7e-06, help: [[-0.6336433  -0.01652482]], reward: -1.0, 100score avg: -192.77\n",
      "episode: 156/70000, score: 199, e 9.7e-06, help: [[-7.85002374e-01 -1.12102588e-04]], reward: -1.0, 100score avg: -192.77\n",
      "episode: 157/70000, score: 167, e 9.7e-06, help: [[0.54110703 0.04419029]], reward: -1.0, 100score avg: -192.45\n",
      "episode: 158/70000, score: 119, e 9.7e-06, help: [[0.54630777 0.04769544]], reward: -1.0, 100score avg: -191.65\n",
      "episode: 159/70000, score: 153, e 9.7e-06, help: [[0.50234186 0.02836065]], reward: -1.0, 100score avg: -191.19\n",
      "episode: 160/70000, score: 199, e 9.7e-06, help: [[-0.49753408  0.01672871]], reward: -1.0, 100score avg: -192.02\n",
      "episode: 161/70000, score: 199, e 9.7e-06, help: [[-0.73006818  0.03178325]], reward: -1.0, 100score avg: -192.6\n",
      "episode: 162/70000, score: 199, e 9.7e-06, help: [[0.22341324 0.00319053]], reward: -1.0, 100score avg: -192.6\n",
      "episode: 163/70000, score: 199, e 9.7e-06, help: [[-0.6417998   0.03028711]], reward: -1.0, 100score avg: -192.6\n",
      "episode: 164/70000, score: 152, e 9.7e-06, help: [[0.50870021 0.03740967]], reward: -1.0, 100score avg: -192.13\n",
      "episode: 165/70000, score: 199, e 9.7e-06, help: [[-0.34583876  0.05686606]], reward: -1.0, 100score avg: -192.13\n",
      "episode: 166/70000, score: 199, e 9.7e-06, help: [[0.24239963 0.03503705]], reward: -1.0, 100score avg: -192.13\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "counter=0 \n",
    "scores_memory= deque(maxlen=100)\n",
    "for e in range(n_episodes):\n",
    "    state=env.reset()\n",
    "\n",
    "    state= np.reshape(state, [1, state_size])\n",
    "    \n",
    "    for time in range(7000):\n",
    "        if e % 50==0:\n",
    "            env.render()\n",
    "        action= agent.act(state)\n",
    "        next_state, reward, done, halp =env.step(action)\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "        if len(agent.memory)>batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            scores_memory.append(time)\n",
    "            scores_avg= np.mean(scores_memory)*-1\n",
    "\n",
    "            \n",
    "            print('episode: {}/{}, score: {}, e {:.2}, help: {}, reward: {}, 100score avg: {}'.format(e, n_episodes, time, agent.epsilon, state, reward, scores_avg))\n",
    "\n",
    "            break\n",
    "    agent.update_target_model()\n",
    "        \n",
    "        \n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "        \n",
    "    if e % 50==0:\n",
    "        agent.save(output_dir + 'weights_final' + '{:04d}'.format(e) + \".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RLmNZ-G4U_d"
   },
   "outputs": [],
   "source": [
    "#DQN and building around OpenAIgym reference:https://www.youtube.com/watch?v=OYhFoMySoVs\n",
    "#Double DQN reference: https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DDQN MountainCar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
